{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install beautifulsoup4\n",
    "%pip install wikipedia-api\n",
    "%pip install google-api-python-client\n",
    "%pip install praw\n",
    "%pip install googlesearch-python\n",
    "%pip install python-dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipediaapi\n",
    "from googleapiclient.discovery import build\n",
    "import praw\n",
    "from googlesearch import search\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre du produit: Chargeur iphone Rapide, certifié Apple MFi 20W Chargeur Rapide USB c pour iphone avec 2m Câble USB C pour Apple iPhone 14/14Pro/13/13 Mini/13 Pro/13 Pro Max/12/12 Pro/12 Pro Max/11/11 Pro/11 Pro Max\n",
      "Prix du produit: 12, \n",
      "Avis sur le produit: 4,2 sur 5 étoiles\n"
     ]
    }
   ],
   "source": [
    "#1. *Amazon:* Web scraping avec BeautifulSoup.  | Produits, prix, avis       |\n",
    "\n",
    "url = 'https://www.amazon.fr/dp/B0BCFJPLT7'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    title = soup.find(id='productTitle').get_text(strip=True)\n",
    "    price = soup.find('span', {'class': 'a-price-whole'}).get_text(strip=True)\n",
    "    avis = soup.find(class_=\"a-icon-alt\").get_text()\n",
    "    \n",
    "    print(f'Titre du produit: {title}')\n",
    "    print(f'Prix du produit: {price} ')\n",
    "    print(f'Avis sur le produit: {avis}')\n",
    "else:\n",
    "    print('La requête a échoué avec le statut:', response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. *Twitter:* Utilisation de l'API Twitter v2. | Tweets, likes, retweets \n",
    "BEARER_TOKEN=os.getenv('BEARER_TOKEN')\n",
    "\n",
    "SEARCH_URL = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "params = {\n",
    "    \"query\": \"data science -is:retweet\",\n",
    "    \"tweet.fields\": \"created_at,public_metrics\",\n",
    "    \"max_results\": 10\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {BEARER_TOKEN}\"\n",
    "}\n",
    "\n",
    "response = requests.get(SEARCH_URL, headers=headers, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    \n",
    "    # Affichage des tweets\n",
    "    for tweet in data.get(\"data\", []):\n",
    "        print(f\"Date : {tweet['created_at']}\")\n",
    "        print(f\"Tweet : {tweet['text']}\")\n",
    "        print(f\"Likes : {tweet['public_metrics']['like_count']}\")\n",
    "        print(f\"Retweets : {tweet['public_metrics']['retweet_count']}\")\n",
    "        print(\"-\" * 50)\n",
    "else:\n",
    "    print(f\"Erreur {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. *Instagram:* API Instagram Graph. | Captions, likes, images\n",
    "ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')\n",
    "INSTAGRAM_ACCOUNT_ID = os.getenv('INSTAGRAM_ACCOUNT_ID')\n",
    "BASE_URL = \"https://graph.facebook.com/v22.0\"\n",
    "\n",
    "def get_instagram_posts(limit=5):\n",
    "    url = f\"{BASE_URL}/{INSTAGRAM_ACCOUNT_ID}/media?fields=id,caption,like_count,comments_count,media_url,permalink,timestamp&limit={limit}&access_token={ACCESS_TOKEN}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            posts = data.get(\"data\", [])\n",
    "\n",
    "            for post in posts:\n",
    "                print(f\"Titre: {post.get('caption', 'Aucune légende')}\")\n",
    "                print(f\"Likes: {post.get('like_count', 0)}\")\n",
    "                print(f\"Commentaires: {post.get('comments_count', 0)}\")\n",
    "                print(f\"Lien du média: {post.get('media_url')}\")\n",
    "                print(f\"Lien du post: {post.get('permalink')}\")\n",
    "                print(f\"Date: {post.get('timestamp')}\")\n",
    "                print('-' * 50)\n",
    "\n",
    "        else:\n",
    "            print(f\"Erreur: {response.status_code}, {response.text}\")\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"La requête a pris trop de temps. Vérifie ta connexion Internet ou essaie plus tard.\")\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Impossible de se connecter. Vérifie ta connexion Internet.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Une erreur est survenue: {e}\")\n",
    "\n",
    "# Exécuter la fonction\n",
    "get_instagram_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: How to Get YouTube API Key 2024 | Create YouTube API Key ( YouTube Data API v3 )\n",
      "comment_count: 132\n",
      "Views: 106595\n"
     ]
    }
   ],
   "source": [
    "#4. *YouTube:* API YouTube Data. | Titres, vues, commentaires\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "VIDEO_ID = os.getenv('VIDEO_ID')\n",
    "\n",
    "def get_video_details(video_id):\n",
    "    \n",
    "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "    request = youtube.videos().list(\n",
    "        part='snippet,statistics',\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    if 'items' in response and len(response['items']) > 0:\n",
    "        video = response['items'][0]\n",
    "        title = video['snippet']['title']\n",
    "        views = video['statistics']['viewCount']\n",
    "        comment_count = video['statistics']['commentCount']\n",
    "\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'comment_count':  comment_count,\n",
    "            'views': views\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "video_details = get_video_details(VIDEO_ID)\n",
    "if video_details:\n",
    "    print(f\"Title: {video_details['title']}\")\n",
    "    print(f\"comment_count: {video_details['comment_count']}\")\n",
    "    print(f\"Views: {video_details['views']}\")\n",
    "else:\n",
    "    print(\"Aucune donnée trouvée pour cette vidéo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: https://www.instagram.com/sara_adjaho/\n",
      "Result 2: https://bj.linkedin.com/in/sara-odile-adjaho-b47399259\n",
      "Result 3: https://www.facebook.com/sara.adjaho.58/\n",
      "Result 4: https://www.facebook.com/Togozik/videos/la-talentueuse-sara-adjaho-sara_adjaho-a-immortalis%C3%A9-le-concert-historique-de-sa/1608590310006912/\n",
      "Result 5: https://www.tiktok.com/@sara.adjaho\n",
      "Result 6: https://genius.com/artists/Sara-adjaho\n",
      "Result 7: https://www.linkedin.com/posts/sara-odile-adjaho-b47399259_activity-7240791877841555456-Wo0i\n",
      "Result 8: https://www.instagram.com/ziktogo/reel/DB43tmTskBK/?locale=ne_NP&hl=af\n",
      "Result 9: https://www.instagram.com/sedonnogniofficial/?locale=fr_CA&hl=af\n",
      "Result 10: https://www.tiktok.com/@sara.adjaho/video/7272090568770047264\n"
     ]
    }
   ],
   "source": [
    "#5. *Google Search:* Scraper avec googlesearch. | Résultats de recherche \n",
    "from googlesearch import search\n",
    "\n",
    "def google_search(query, num_results=10):\n",
    "    try:\n",
    "       \n",
    "        search_results = search(query, num_results=num_results)\n",
    "        \n",
    "        for i, result in enumerate(search_results, start=1):\n",
    "            print(f\"Result {i}: {result}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite: {e}\")\n",
    "\n",
    "\n",
    "query = \"sara adjaho\"\n",
    "google_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre: Sunday Daily Thread: What's everyone working on this week?\n",
      "Votes: 8\n",
      "Lien: https://www.reddit.com/r/Python/comments/1jhmdi1/sunday_daily_thread_whats_everyone_working_on/\n",
      "Commentaires:\n",
      "  - Currently I’m working on another text based game. I don’t have a name for it but it’s build like an rpg. The UI is in handmade ASCII and is displayed via the console. All you would need is something like VSC to run it. The user has 9 equipment slots. Head, neck 1, neck 2, torso, gloves, pants, boots, main hand, off hand. Each one can have 0-3 affixes applies to it. The affix list for each item varies. Currently I have it sorted into a couple categories. Generic, Armor Generic, and Weapon specific. I’m still working on the gear and affix system. I’m at around 800~ and I’m almost done with the affixes. Still working on all the gear types. (Votes: 2)\n",
      "  - I'm working on a framework for local/on-prem small-data processing/ETL and warehousing.  \n",
      "\n",
      "__Background__  \n",
      "I only deal with a couple thousand rows of data between a half dozen spreadsheets each month, and my team is only me. This project was inspired by my ad-hoc scripts becoming unwieldy, but not enough to pay for anything, or to deploy a more enterprise-level solution (e.g. Dagster, Databricks, etc.)\n",
      "\n",
      "__Features__  \n",
      "It uses a decorator-API to wrap your Extract, Transform, and Load functions (which basically monkey-patch the user-defined functions as methods on the `Datasource` instances). It has support for metadata and logging, and is quite flexible. A simple example looks something like this:  \n",
      "\n",
      "\n",
      "```python\n",
      "import polars as pl\n",
      "...\n",
      "\n",
      "from my_project import Datasource\n",
      "\n",
      "my_data = Datasource(\n",
      "    name=\"MyData\",\n",
      "    filename=r\"C:/folder/*report.xlsx\",\n",
      "    ...\n",
      ")\n",
      "\n",
      "\n",
      "@my_data.extract_wrapper\n",
      "def extract(datasource, *args, **kwargs) -> pl.DataFrame:\n",
      "    df = pl.read_excel(datasource.filename, ...)\n",
      "    datasource.logger.info(\"Loaded raw data\")\n",
      "    return df  # Sets my_data.raw_data\n",
      "\n",
      "\n",
      "@my_data.transform_wrapper\n",
      "def transform(datasource, *args, **kwargs) -> pl.DataFrame \n",
      "    df = datasource.raw_data ...  # do transformations\n",
      "    return df  # Sets my_data.data\n",
      "\n",
      "\n",
      "# load function, etc.\n",
      "\n",
      "```\n",
      "\n",
      "I'm also working on the `Warehouse` object (a wrapper around a DuckDB database) that has functionality to parse SQL files for dependencies and sort them (the files) using `graphlib` before execution. I think this means that I'm employing \"DAGs\" and I am therefore hip lol.  \n",
      "\n",
      "Finally, there's a command line utility that rebuilds the database/warehouse using your `Datasource`'s, executing their ETL functions and running (sorted) SQL scripts. It can can run also run `Reports` (WIP).  \n",
      "\n",
      "Any comments are appreciated!  \n",
      "(Be critical; ClaudeAI has given me enough ego boosts lol) (Votes: 1)\n",
      "  - Using flask and dns\\_lib to create a DDNS server to resolve any hostname of a CCTV NVR (Hikvision NVRs mostly) to non static IPs. (Votes: 1)\n",
      "--------------------------------------------------\n",
      "Titre: Thursday Daily Thread: Python Careers, Courses, and Furthering Education!\n",
      "Votes: 3\n",
      "Lien: https://www.reddit.com/r/Python/comments/1jkqpgl/thursday_daily_thread_python_careers_courses_and/\n",
      "Commentaires:\n",
      "--------------------------------------------------\n",
      "Titre: Hot Module Replacement in Python\n",
      "Votes: 11\n",
      "Lien: https://www.reddit.com/r/Python/comments/1jl8azv/hot_module_replacement_in_python/\n",
      "Commentaires:\n",
      "  - With Python imports not being side effect free this post raises more questions than answers for me... (Votes: 16)\n",
      "  - I'd love to learn more about what you're building. Sounds substantial if reload times are getting in your way.\n",
      "\n",
      "In the past, I've seen this dealt with by modularizing and selectively loading (and/or lazy-loading) components under test. It also helps on the deployment side so an app written as, say, a huge ~million-line Django monolith, can be independently deployed and independently scaled according to its modularized component sets. Though you have to be rigorously diligent about inter-dependencies. Conveniently, Django already has a strong concept of independent apps within a single project, but other frameworks may not be so lucky.\n",
      "\n",
      "HMR feels bad in my mind, but maybe that's only because everyone who has tried it before deemed it a horrible idea. It's hard to imagine the benefits being worth the possible pitfalls in general cases, but maybe you've really got a reason for this if you're seeing minute+ reload times without it. (Votes: 1)\n",
      "  - Not sure it is possible with shared objects (some_native_library.so) (Votes: 1)\n",
      "  - Is it 2006 again ? This is crazy, just put an ingress on front and let kubernetes do that for you. Hot reloading is insane on production, and all of this is some crazy effort impossible to justify for local development. Like other say fix any side effect making module loading slow in the first place. (Votes: 1)\n",
      "  - And that's why sometimes I want to strangle the engineers that make such modules. All you do is an import, and what you get is a database connection or two, schema migration processes starting, everything is loaded up into memory and decisions made based on that which configs to load and where to dump the whole thing, global variables are defined, and functions that read those global variables are called. Over 9000 errors get triggered if everything is not perfectly set up. And all I wanted was to write a unit test for a stupid function somewhere. (Votes: 13)\n",
      "--------------------------------------------------\n",
      "Titre: excel-serializer: dump/load nested Python data to/from Excel without flattening\n",
      "Votes: 122\n",
      "Lien: https://www.reddit.com/r/Python/comments/1jkkjvw/excelserializer_dumpload_nested_python_data/\n",
      "Commentaires:\n",
      "  - Great stuff, I know just the data to test this on. (Votes: 12)\n",
      "  - I'm interested in trying this as it's something I do all the time. (Votes: 10)\n",
      "  - That's weird.  I usually try to do stuff the other way around.  Why'd you want to put something *into* excel?  Then you'd have to use excel ;) (Votes: 8)\n",
      "  - If you properly combine pydantic for schema validation/prep this can probably be used as an ORM. Possibly replacing the need for a database for small amounts of data. I like the idea, see if I can put this to the test. (Votes: 3)\n",
      "  - What’s up with the values in the ID column in the example? (Votes: 3)\n",
      "--------------------------------------------------\n",
      "Titre: Working on better PDF APIs at Foxit. Python folks, what would you actually want?\n",
      "Votes: 47\n",
      "Lien: https://www.reddit.com/r/Python/comments/1jkpvsz/working_on_better_pdf_apis_at_foxit_python_folks/\n",
      "Commentaires:\n",
      "  - Gonna be honest, I would do a lot to avoid using an api for anything pdf related and much prefer doing things locally (Votes: 39)\n",
      "  - If you mean web API, I would never use it, I'm more interested in doing things locally (local web server is also ok) (Votes: 34)\n",
      "  - Why make this an api? I have to be dependent on your service and a network. Anyone who's smart enough to know what an API is would much rather just use a library that can run locally (Votes: 21)\n",
      "  - APIs work as endpoints and integrations but should not implement core functionality. Otherwise there’s nothing to stop the API provider from doing a sudden rug pull and leaving you out to dry. (Votes: 17)\n",
      "  - It’s a massive pain to have to use an internet facing service in my industry. There is really no need for this to be hosted. Our IT team can install software. (Votes: 7)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#6. *Reddit:* API Reddit (PRAW). | Posts, votes, commentaires\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv('CLIENT_ID'),\n",
    "    client_secret=os.getenv('CLIENT_SECRET'),\n",
    "    user_agent='Data science'\n",
    ")\n",
    "\n",
    "\n",
    "subreddit_name = 'python'\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "for submission in subreddit.hot(limit=5):\n",
    "    print(f'Titre: {submission.title}')\n",
    "    print(f'Votes: {submission.score}')\n",
    "    print(f'Lien: {submission.url}')\n",
    "    print('Commentaires:')\n",
    "\n",
    "    submission.comment_sort = 'top'\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    \n",
    "    for comment in submission.comments.list()[:5]:\n",
    "        print(f'  - {comment.body} (Votes: {comment.score})')\n",
    "\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. *Wikipedia:* API Wikipédia. | Contenu d'articles.       \n",
    "def extract_wikipedia(page_title,language='en'):\n",
    "    headers ={\n",
    "        \"User-Agent\": \"pythonscripte/1.0 (adjahosarahouefa@gmail.com)\"\n",
    "    }\n",
    "    page_wiki = wikipediaapi.Wikipedia(language, headers=headers).page(page_title)\n",
    "    \n",
    "    if not page_wiki.exists():\n",
    "        return f\"Page '{page_title}' not found.\"\n",
    "    \n",
    "    return page_wiki.text\n",
    "  \n",
    "page_title = \"Bénin\"\n",
    "content = extract_wikipedia(page_title)\n",
    "print(content)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
